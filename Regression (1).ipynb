{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?   \n",
        "Simple Linear Regression is a statistical method used to model the relationship between two variables:   \n",
        "   One independent variable (X) – also called predictor or input  \n",
        "   One dependent variable (Y) – also called response or output  \n",
        "  Purpose: To find a straight line (linear equation) that best describes how Y changes with X."
      ],
      "metadata": {
        "id": "HpMkeKk8PSQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?  \n",
        "The key assumptions of Simple Linear Regression ensure that the model is valid and its predictions are reliable. Here are the 5 core assumptions:  \n",
        "  1- Linearity- The relationship between the independent variable (X) and the dependent variable (Y) is linear.  \n",
        "  This means that the change in Y is proportional to the change in X.  \n",
        "  2- Independence of Errors- The residuals (errors) should be independent of each other.  \n",
        "  Common violation: In time series data, where today's error is related to yesterday's.  \n",
        "  3- Homoscedasticity- The variance of residuals should be constant across all values of X.  \n",
        "  If variance increases or decreases with X, the model is heteroscedastic.  \n",
        "  4- Normality of Errors- The residuals (differences between actual and predicted Y) should be normally distributed.  \n",
        "  Important for making confidence intervals and hypothesis tests.  \n",
        "  5- No Multicollinearity- In Simple Linear Regression, this doesn’t apply since there is only one predictor."
      ],
      "metadata": {
        "id": "HT38Q824PSMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y=mX+c?  \n",
        "   In the linear equation:\n",
        "𝑌\n",
        "=\n",
        "𝑚𝑋+\n",
        "𝑐\n",
        "  \n",
        "    The coefficient 𝑚 represents the slope of the line.  \n",
        "    Meaning of the Slope (\n",
        "𝑚\n",
        " ):\n",
        "It shows the rate of change of\n",
        "𝑌 with respect to\n",
        "𝑋\n",
        ".\n",
        "  In simple terms, it tells you how much 𝑌 increases or decreases when 𝑋 increases by 1 unit.\n",
        "\n"
      ],
      "metadata": {
        "id": "wFO2In4ePSC_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y=mX+c?  \n",
        "   In the linear equation: 𝑌=𝑚𝑋+ 𝑐  \n",
        "   The intercept 𝑐 represents the value of 𝑌 when 𝑋 = 0\n",
        "\n",
        "   It is also called the Y-intercept because it is the point where the line crosses the Y-axis.  \n",
        "\n",
        "   Meaning of the Intercept (𝑐): It tells you the starting value of 𝑌 before any influence of 𝑋.  \n",
        "   It’s the predicted value of 𝑌 when the independent variable 𝑋 is zero.\n",
        "\n"
      ],
      "metadata": {
        "id": "SrTk_vLAPR_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?  \n",
        "   In Simple Linear Regression, the slope\n",
        "𝑚\n",
        "m (also called coefficient\n",
        "𝑏\n",
        "1\n",
        "​\n",
        " ) represents how much the dependent variable\n",
        "𝑌 changes for a unit change in the independent variable\n",
        "𝑋.  \n",
        "\n",
        "    m= ∑(X\n",
        "i − Xˉ)(Yi − Yˉ) / ∑(Xi − Xˉ)2  \n",
        "\n",
        "   Where: 𝑋𝑖, 𝑌𝑖 = individual data points   \n",
        "     Xˉ, 𝑌ˉ= mean of X and 𝑌  \n",
        "     m = slope of the regression line\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fumhBpyBPR86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?  \n",
        "The purpose of the least squares method in Simple Linear Regression is to find the best-fitting straight line (also called the regression line) through a set of data points. This method minimizes the sum of the squares of the vertical differences (also called residuals) between the observed values (actual data points) and the values predicted by the linear model.  \n",
        "𝑏 y=mx+b that makes the predictions as close as possible to the actual data points by minimizing the total squared error."
      ],
      "metadata": {
        "id": "O0WcNZ16PR6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?  \n",
        "In Simple Linear Regression, the coefficient of determination (R²) is a key metric used to evaluate how well the regression line fits the data. Here’s how it’s interpreted:  \n",
        "R² measures the proportion of the variance in the dependent variable (Y) that is explained by the independent variable (X) using the linear model.\n",
        "\n",
        " Interpretation: R² = 1 → Perfect fit: 100% of the variance in Y is explained by X.\n",
        "\n",
        "  R² = 0 → The model explains none of the variability of the response data around its mean.\n",
        "\n",
        "  0 < R² < 1 → The model explains part of the variability, e.g.:\n",
        "\n",
        "  R² = 0.75 → 75% of the variation in Y is explained by X\n",
        "\n",
        "  R² = 0.30 → 30% of the variation in Y is explained by X\n",
        "  \n",
        "   Why It’s Useful: Gives a quantitative measure of model performance\n",
        "\n",
        "  Helps compare multiple models (though with caution)\n",
        "\n",
        "  Can signal underfitting (if R² is very low)\n",
        "\n",
        " Things to Keep in Mind: High R² ≠ good model always (could be overfitting or due to outliers)\n",
        "\n",
        " Only valid for linear relationships (in linear regression)\n"
      ],
      "metadata": {
        "id": "35-t3bwvPRvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Multiple Linear Regression?  \n",
        " Multiple Linear Regression is an extension of Simple Linear Regression used to model the relationship between one dependent variable and two or more independent variables.  \n",
        " It estimates the linear relationship between: One dependent variable\n",
        "𝑌 and Multiple independent variables 𝑋1,𝑋2,...,𝑋𝑛X1,X2,...,Xn\n",
        "  \n",
        "  Equation of Multiple Linear Regression: Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        "​\n",
        " +ε"
      ],
      "metadata": {
        "id": "wMhhSwwmPRru"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?  \n",
        "   Simple Linear Regression is used when you want to predict a dependent variable using just one independent variable. For example, if you want to predict a person's weight based only on their height, that's simple linear regression. The formula looks like this:  \n",
        "       Y=b0 + b1X  \n",
        "   Where 𝑌 is the outcome (like weight), and 𝑋 is the single input (like height).   \n",
        "\n",
        "    Multiple Linear Regression is used when the prediction depends on two or more independent variables. For example, if you're predicting someone's weight based on height, age, and activity level, you'd use multiple linear regression. Its formula expands to:  \n",
        "      Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        "​\n",
        " +…   \n",
        " So, the key difference is the number of inputs: Simple has one, Multiple has more than one.  "
      ],
      "metadata": {
        "id": "wGQ5Y03LPRm7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?  \n",
        "    Multiple Linear Regression shares many of the same assumptions as Simple Linear Regression, but with additional attention to relationships among multiple independent variables. These assumptions ensure that the model's results are valid and interpretable.   \n",
        "    1. Linearity- There should be a linear relationship between the dependent variable and each independent variable.  \n",
        "    Why it matters: If the relationship is not linear, the model’s predictions will be biased.  \n",
        "    2. Independence of Errors- The residuals (errors) should be independent — one error should not depend on another.  \n",
        "    Example violation: Time series data where one observation influences the next.  \n",
        "    3. Homoscedasticity (Constant Variance of Errors)- The variance of residuals should be constant across all levels of the independent variables.  \n",
        "    Violation symptom: You see a “funnel shape” in residual plots (errors spread more at high or low values).  \n",
        "    4. Normality of Residuals- The residuals (differences between actual and predicted values) should be approximately normally distributed.  \n",
        "    Why it matters: This is important for constructing confidence intervals and performing hypothesis tests.  \n",
        "    5. No Multicollinearity- The independent variables should not be highly correlated with each other.  \n",
        "    6. No Autocorrelation (for time-based data)- In time series or ordered data, the residuals should not be correlated with each other.  \n",
        "    7. Measurement Accuracy- Independent variables should be measured without error. In real life, this is hard to guarantee but is assumed for theoretical accuracy."
      ],
      "metadata": {
        "id": "vvPun1yuPRkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?  \n",
        "   Heteroscedasticity occurs when the variance of the residuals (errors) in a regression model is not constant across all levels of the independent variables.  \n",
        "   In simple terms, this means that for some values of the predictor(s), the model’s errors are larger or smaller than for others. It violates one of the key assumptions of Multiple Linear Regression, which assumes homoscedasticity — that is, equal variance of the residuals.  \n",
        "\n",
        "   How Heteroscedasticity Affects a Regression Model:  \n",
        "   1. Unreliable Standard Errors: The biggest issue is that standard errors of the coefficients become biased.This leads to incorrect p-values and confidence intervals, making hypothesis tests unreliable.  \n",
        "  2. Misleading Significance Tests: You might think a variable is statistically significant when it isn’t, or vice versa.  \n",
        "  3. Inefficient Coefficients: The regression still gives unbiased estimates of the coefficients (β), but they are no longer efficient — meaning not the “best” (lowest variance).  \n",
        "  4. Poor Predictive Performance: If the error variance is large in certain ranges of your predictor, the model might perform poorly there."
      ],
      "metadata": {
        "id": "ytoT6NANPRiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?  \n",
        "   To improve a Multiple Linear Regression model suffering from high multicollinearity, you need to address the issue of highly correlated independent variables, which can make the model unstable and reduce the reliability of coefficient estimates.  \n",
        "   Multicollinearity--Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning they contain similar information about the variance in the dependent variable.  \n",
        "\n",
        "   Ways to Improve the Model--  \n",
        "   1- Check for Multicollinearity  \n",
        "   2- Remove Highly Correlated Predictors  \n",
        "   3- Combine Predictors (Feature Engineering)  \n",
        "   4- Use Principal Component Analysis (PCA)  \n",
        "   5- Apply Regularization Techniques  \n",
        "   6- Centering Variables (Mean Normalization)"
      ],
      "metadata": {
        "id": "QXAsS7_6PRgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression models?  \n",
        "    To use categorical variables in regression models, you need to transform them into a numerical format since regression models require numerical input. Here are the most common techniques for transforming categorical variables:  \n",
        "\n",
        "    1. One-Hot Encoding (Dummy Variables)- Creates a new binary column for each category.  \n",
        "   Value is 1 if the observation belongs to the category, else 0.  \n",
        "   Drop one column to avoid dummy variable trap (perfect multicollinearity).  \n",
        "    2. Label Encoding- Assigns each category an integer value (e.g., Red = 0, Blue = 1, Green = 2).  \n",
        "   Simple but imposes ordinal meaning that may not be appropriate.  \n",
        "    3. Ordinal Encoding- Explicitly assigns ordered integers to ordered categories.  \n",
        "    4. Target Encoding (Mean Encoding)- Replace each category with the mean of the target variable for that category.  \n",
        "    5. Binary Encoding- Combination of label encoding and binary representation.  \n",
        "    Each category is assigned a binary number, which is then split into columns.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u85x95EWPReD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?  \n",
        "    Interaction terms are used to capture the effect of two or more variables acting together on the target variable — when the effect of one predictor depends on the value of another.  \n",
        "\n",
        "    In standard multiple linear regression, the model assumes that the predictors have independent and additive effects on the outcome:  \n",
        "       Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2+ϵ  \n",
        "    But sometimes, the effect of 𝑋1 on 𝑌 changes depending on 𝑋2​.  \n",
        "  \n",
        "  In that case, we introduce an interaction term:    \n",
        " Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        "​\n",
        " +β\n",
        "3\n",
        "​\n",
        " (X\n",
        "1\n",
        "​\n",
        " ⋅X\n",
        "2\n",
        "​\n",
        " )+ϵ  \n",
        " The term\n",
        "𝛽\n",
        "3\n",
        "(\n",
        "𝑋\n",
        "1\n",
        "⋅\n",
        "𝑋\n",
        "2\n",
        ")\n",
        "β\n",
        "3\n",
        "​\n",
        " (X\n",
        "1\n",
        "​\n",
        " ⋅X\n",
        "2\n",
        "​\n",
        " ) captures the interaction effect.  \n",
        "\n",
        "    Why Use Interaction Terms-- 1. Model Real-World Complexity- Real-world relationships are rarely purely additive.    \n",
        "    2.Improve Predictive Accuracy- Including interaction terms can reduce residual errors when interactions truly exist.  \n",
        "    3.Reveal Insights- Helps uncover patterns like “Feature A only matters when Feature B is high.\n"
      ],
      "metadata": {
        "id": "qxAzKT20PRbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?  \n",
        "   In simple linear regression, the intercept is the value of the dependent variable (Y) when the independent variable (X) is zero. For example, if you're predicting someone's salary based on years of experience, the intercept would represent the expected salary for someone with zero years of experience. It’s usually easy to understand and interpret in simple regression.  \n",
        "   In multiple linear regression, where you have two or more independent variables, the intercept is the predicted value of Y when all the independent variables are zero. This can be hard to interpret in real life because having all variables equal to zero may not make sense. For example, if you're predicting house prices based on square footage, number of rooms, and location score, then the intercept would be the predicted price of a house with 0 square feet, 0 rooms, and a location score of 0 — which likely doesn’t exist. So, in multiple regression, the intercept is still part of the model but often doesn’t have a practical meaning on its own."
      ],
      "metadata": {
        "id": "lRzGQL_yPRZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?  \n",
        "   In regression analysis, the slope shows the relationship between an independent variable (X) and the dependent variable (Y). It tells us how much Y is expected to change when X increases by one unit, while keeping all other variables constant (in multiple regression).  \n",
        "   If you have a regression equation like: Y=b0 + b1X  \n",
        "\n",
        "   What does it mean- If the slope is positive, it means that as X increases, Y also increases.  \n",
        "   If the slope is negative, then as X increases, Y decreases.  \n",
        "   A slope of 0 means there’s no relationship — X has no effect on Y.  \n",
        "\n",
        "   Why is the slope important- The slope shows how strong and in what direction a variable affects the prediction.\n",
        "   For example, in predicting house prices: If the slope of “square feet” is 3000, it means each additional square foot increases the price by ₹3000.\n",
        "\n",
        "   In multiple regression: Each variable has its own slope, showing how that one feature affects the outcome, while keeping all other variables the same. So the slope is crucial for predictions — it determines how the output will change when input values change.\n",
        "\n"
      ],
      "metadata": {
        "id": "yEVrMf7TPRXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?  \n",
        "   The intercept in a regression model provides the starting point or baseline of the relationship between the variables. It tells us the predicted value of the dependent variable (Y) when all independent variables (X) are zero.  \n",
        "   the regression equation: Y=b0 + b1X  \n",
        "    (𝑏0 is the intercept., It shows what Y would be when X = 0.)  \n",
        "   So, the intercept gives context by showing where the prediction starts before any effect from the independent variables is added.  \n",
        "\n",
        "   In Multiple Regression: The intercept is the predicted value of Y when all input variables are zero. It may or may not make practical sense (e.g., a house with 0 size, 0 rooms, etc.), but mathematically, it anchors the equation."
      ],
      "metadata": {
        "id": "TWhp5-3IPRVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R² as a sole measure of model performance?  \n",
        "   Using R² (R-squared) as the only measure of model performance has several important limitations, especially in real-world regression problems. R² tells us how much of the variation in the target variable (Y) is explained by the model.  \n",
        "\n",
        "   Limitations of Using R² Alone:  \n",
        "   1- R² doesn’t show if the model is good for prediction   \n",
        "   2- R² doesn’t indicate whether the relationships are meaningful  \n",
        "   3- Adding more variables always increases R²  \n",
        "   4- R² doesn’t detect multicollinearity  \n",
        "   5- R² is not suitable for comparing different types of models  \n",
        "   6- R² doesn’t capture error distribution"
      ],
      "metadata": {
        "id": "paxoFotlPRTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?  \n",
        "    A large standard error for a regression coefficient means that the estimate of that coefficient is not precise — the model is uncertain about the true effect of that variable.  \n",
        "    Standard Error of a Coefficient-- It measures how much the estimated coefficient might vary if you repeated the sampling process.  \n",
        "    A small standard error means the coefficient estimate is stable and reliable.  \n",
        "    A large standard error means the estimate could change a lot — it's less trustworthy.  \n",
        "\n",
        "    What Does a Large Standard Error Indicate--  \n",
        "    1- The coefficient might not be statistically significant  \n",
        "    2- Wide confidence intervals  \n",
        "    3- Multicollinearity might be present  \n",
        "    4- Insufficient data or high noise"
      ],
      "metadata": {
        "id": "vhszExaiPRQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?  \n",
        "    Heteroscedasticity Be Identified in Residual Plots- Heteroscedasticity means the spread (variance) of residuals is not constant across all levels of the predicted values or independent variables. To identify it:  \n",
        "    Use a residual vs. fitted values plot: X-axis: predicted (fitted) values and Y-axis: residuals (errors = actual - predicted)  \n",
        "    In a good model (homoscedasticity), residuals are evenly spread out, forming a random horizontal band.   \n",
        "    In a heteroscedastic model, you’ll see patterns like: A funnel shape (residuals spread out as values increase or decrease), A cone shape, curve, or systematic structure  \n",
        "\n",
        "    Why Is It Important to Address Heteroscedasticity--\n",
        "    1. Unreliable Standard Errors- Coefficient estimates may still be correct, but their standard errors are wrong → leads to incorrect p-values and confidence intervals.  \n",
        "    2. Invalid Hypothesis Tests- You may wrongly conclude a variable is significant (or not) due to misleading significance levels.  \n",
        "    3. Poor Model Performance in Certain Ranges- If errors are larger for higher values, predictions will be less accurate in those areas.  \n",
        "    4. Violates Regression Assumptions- Linear regression assumes constant error variance. Violating this affects the model's validity and interpretability."
      ],
      "metadata": {
        "id": "sCzfjgeZPROT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?  \n",
        "    If a Multiple Linear Regression model has a high R² but a low Adjusted R², it means that the model appears to explain a lot of the variance in the target variable, but not in a meaningful or efficient way.  \n",
        "    What’s the difference--  \n",
        "    R² shows how much of the variation in the dependent variable is explained by the model.  \n",
        "    Adjusted R² does the same but penalizes the model for using too many predictors, especially if those predictors don't add real value.   \n",
        "\n",
        "    So if R² is high but Adjusted R² is low:  \n",
        "    The model has too many predictors, some of which are irrelevant or weak.  \n",
        "    These extra variables are inflating the R² artificially.  \n",
        "    The model may be overfitting the training data and won't perform well on new data."
      ],
      "metadata": {
        "id": "DVGYKygdPRM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?  \n",
        "    Scaling variables in Multiple Linear Regression is important when your predictors (independent variables) have different units or ranges, especially if you’re using techniques that depend on magnitude comparisons, like regularization or interpreting coefficient size.\n",
        "    Why Scaling Matters---   \n",
        "    1. Improves interpretability of coefficients-  When variables are on very different scales, the size of the coefficients can be misleading. A large coefficient might not mean a stronger effect — it might just be compensating for a small-scaled variable.  \n",
        "    2. Essential for regularization (like Ridge or Lasso)- Regularized models penalize large coefficients: If one variable has a large scale, its coefficient will be smaller to avoid penalty — even if it’s important.  \n",
        "    Scaling ensures fair treatment of all variables during the regularization process.  \n",
        "    3. Helps with numerical stability-Scaling reduces computational errors during matrix operations in model fitting. It’s especially important in large datasets with high variance in feature magnitudes.  \n",
        "    4. Allows better model convergence- Some optimization algorithms (like gradient descent) perform better and converge faster when inputs are on similar scales.\n",
        "  "
      ],
      "metadata": {
        "id": "6o5SuhYNPRKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?  \n",
        "    Polynomial regression is a type of regression analysis where the relationship between the independent variable 𝑋 and the dependent variable 𝑌 is modeled as an nth-degree polynomial rather than a straight line.  \n",
        "    in polynomial regression, you allow curves by including powers of 𝑋:  \n",
        "    Y=b\n",
        "0\n",
        "​\n",
        " +b\n",
        "1\n",
        "​\n",
        " X+b\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +b\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+b\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ"
      ],
      "metadata": {
        "id": "70yKJamyPRIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?  \n",
        "    Polynomial regression and linear regression are both used to model the relationship between a dependent variable and one or more independent variables — but they differ in how they model that relationship.  \n",
        "    Main Difference--  \n",
        "    Linear regression models a straight-line relationship between the independent variable(s) and the target.  \n",
        "    Polynomial regression models a curved (non-linear) relationship by adding powers of the independent variable(s)."
      ],
      "metadata": {
        "id": "QLeZD7v_PQ5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression used?  \n",
        "    Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear, but can be well-approximated by a polynomial curve.  \n",
        "    use polynomial regression when  \n",
        "    1. The data shows a curved pattern-- If your scatter plot of X vs. Y shows a U-shape, inverted U-shape, or wave-like curve, linear regression won’t fit well — a polynomial curve can capture that shape.  \n",
        "    2. Linear regression leaves patterned residuals-- If a linear model’s residual plot shows a clear pattern (not random scatter), it means a linear model isn’t capturing the full relationship — a polynomial regression might help.   \n",
        "    3. You want to model interactions or non-constant growth-- growth that increases slowly, then rapidly, or peaks and drops.\n"
      ],
      "metadata": {
        "id": "IQFFOHZ7jH_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?  \n",
        "   The general equation for polynomial regression models the relationship between the independent variable X and the dependent variable 𝑌 as a polynomial function of degree 𝑛.  \n",
        "\n",
        " Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ  \n",
        " where:  \n",
        " 𝑌 = dependent variable (what you’re predicting)  \n",
        " 𝑋 = independent variable  \n",
        " β0 = intercept (constant term)  \n",
        " 𝛽\n",
        "1\n",
        ",\n",
        "𝛽\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝛽\n",
        "𝑛, β\n",
        "2,…,β\n",
        "n\n",
        "​\n",
        "  = coefficients for each power of 𝑋  \n",
        "  𝑛 = degree of the polynomial  \n",
        "  𝜖 = error term (difference between actual and predicted values)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3tL07UyWjH76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple variables?   \n",
        "    Yes, polynomial regression can be applied to multiple variables — this is called multivariate polynomial regression.  \n",
        "\n",
        "    Just like in regular multiple linear regression where you have multiple input variables (X₁, X₂, X₃...), in multivariate polynomial regression, you can also include squared, cubed, and interaction terms of those variables."
      ],
      "metadata": {
        "id": "GSaFeZ1hjH3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?  \n",
        "     1. Overfitting-   \n",
        "     As the degree of the polynomial increases, the model becomes more flexible.  \n",
        "     While this may fit the training data very well, it can lead to poor performance on new (test) data.  \n",
        "     The curve might follow the noise in the data instead of the actual pattern.  \n",
        "\n",
        "    2. Extrapolation is unreliable-  \n",
        "    Polynomial models can behave unpredictably outside the range of the training data.  \n",
        "    Even a good curve within known data may shoot off wildly when predicting unseen values.  \n",
        "    \n",
        "    3. High variance with high-degree polynomials-  \n",
        "    Small changes in the data can result in big changes in the model’s predictions.  \n",
        "    The model becomes sensitive and unstable.  \n",
        "\n",
        "    4. Increased complexity-  \n",
        "     Adding higher-degree terms or interaction terms in multivariate polynomial regression increases the number of features rapidly, leading to: Computational cost, Difficulty in interpretationHigher chance of multicollinearity  \n",
        "\n",
        "    5. Diminishing interpretability-  \n",
        "      A linear model is easy to understand  \n",
        "      Polynomial models are harder to explain, especially to non-technical audiences.  \n",
        "\n",
        "   6. Assumes a specific functional form-  \n",
        "   If the true relationship is not polynomial-shaped, forcing a polynomial model may still give poor results.  \n",
        "   It’s not a flexible non-parametric method like decision trees or splines.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i75uupjjjH0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?  \n",
        "    When selecting the degree of a polynomial in polynomial regression, it’s important to choose a degree that balances model complexity and predictive performance. Here are the most effective methods for evaluating model fit to help you select the right degree:  \n",
        "    1. Visual Inspection (Plot the Curve)-  Plot the data and the fitted polynomial curve.  \n",
        "    If the model is too simple, it will underfit (miss curves in the data).  \n",
        "    If it's too complex, it will overfit (wiggle too much).  \n",
        "    2. Train-Test Split (or Holdout Validation)- Split the dataset into training and test sets.  \n",
        "    Fit polynomial models of various degrees on the training set.  \n",
        "    Compare their Mean Squared Error (MSE) or R² on both sets.  \n",
        "    3. Cross-Validation (e.g., K-Fold CV)- More reliable than a single train-test split.  \n",
        "    Fit multiple polynomial models (degrees 1, 2, 3...) and compute average validation error across folds.  \n",
        "    4. Adjusted R²- Unlike plain R², Adjusted R² penalizes for adding unnecessary polynomial terms.  \n",
        "    A degree that increases adjusted R² significantly is likely useful."
      ],
      "metadata": {
        "id": "jijnMYsjjHpp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?  \n",
        "    Visualization is very important in polynomial regression because it helps you clearly see whether the model is doing a good job of capturing the true relationship between the variables — especially when the relationship is non-linear.  \n",
        "    1. Reveals the Shape of the Data  \n",
        "    2. Helps You Choose the Right Degree  \n",
        "    3. Makes Model Errors Clear  \n",
        "    4. Improves Understanding and Communication  \n",
        "    5. Identifies Outliers and Influential Points"
      ],
      "metadata": {
        "id": "VHLqlLcOm1GA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?  \n",
        "    Polynomial regression is easy to implement in Python using scikit-learn. It involves transforming the input features into polynomial features and then applying linear regression on the transformed data.  \n",
        "\n",
        "    Step 1: Import Libraries  \n",
        "    Step 2: Create Sample Data  \n",
        "    Step 3: Transform Features into Polynomial Form  \n",
        "    Step 4: Fit a Linear Regression Model  \n",
        "    Step 5: Make Predictions and Plot"
      ],
      "metadata": {
        "id": "rbnTppZJm08o"
      }
    }
  ]
}